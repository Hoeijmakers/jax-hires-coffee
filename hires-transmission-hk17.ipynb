{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79c8f2a-94d1-4065-a9ec-e4dabca8ba2f",
   "metadata": {},
   "source": [
    "# Applying [JAX](https://github.com/google/jax) to a model with realistic noise, continuum filtering and wavelength sampling.\n",
    "Brett Morris & Jens Hoeijmakers\n",
    "\n",
    "\n",
    "This is based on a fork of Brett's retrieval demo, applied to high resolution transmission spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dd5015-6d22-4668-b2d1-112c749e8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = False#Set this to False to enable plotting. Set to True and export the script to perform a batch calcuation.\n",
    "#In this case, all in-between plotting and speed tests are skipped.\n",
    "\n",
    "if not batch:\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "# We need to import numpyro first, though we use it last\n",
    "import numpyro\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "from numpyro import distributions as dist\n",
    "\n",
    "cpu_cores = 4\n",
    "numpyro.set_host_device_count(cpu_cores)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.interpolate as interp\n",
    "from jax import numpy as jnp\n",
    "from jax import jit\n",
    "from jax.scipy.optimize import minimize\n",
    "from jax.random import PRNGKey, split\n",
    "from functools import partial\n",
    "import arviz\n",
    "from corner import corner\n",
    "\n",
    "import astropy.io.fits as fits\n",
    "import astropy.constants as const\n",
    "import astropy.units as u  \n",
    "\n",
    "import tayph.util as ut\n",
    "import tayph.system_parameters as sp\n",
    "import tayph.functions as fun\n",
    "import tayph.util as ut\n",
    "from tayph.vartests import typetest,notnegativetest,nantest,postest,typetest_array,dimtest\n",
    "from tayph.vartests import lentest\n",
    "import tayph.operations as ops\n",
    "import tayph.masking as masking\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import jax\n",
    "rng_seed = 42\n",
    "rng_keys = split(PRNGKey(rng_seed),cpu_cores)\n",
    "\n",
    "if batch: \n",
    "    print(f'Starting script in batch mode on {cpu_cores} cores.')\n",
    "    \n",
    "order_start = 0\n",
    "order_end = 15\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f7557-5d5a-44d3-86d2-3314eb8aa0d7",
   "metadata": {},
   "source": [
    "## Loading opacities.\n",
    "Opacity functions of various species are located in the `opacity/` folder. We load them using a binary IO script packaged in `tayph`. We save each species in a species object, and keep track of those with a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72040078-3cdd-4299-8dd8-681f882b262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class species:#Species\n",
    "  def __init__(self, label, tag):\n",
    "    self.label = label\n",
    "    self.tag = tag\n",
    "    \n",
    "\n",
    "# labels = ['Ca', 'Ti', 'V', 'Cr', 'Fe']\n",
    "# tags = [2000,2200,2300,2400,2600]\n",
    "\n",
    "labels=['Fe','Ti','V']\n",
    "tags=[2600,2200,2300]\n",
    "S = OrderedDict()#This will hold all my species objects.\n",
    "for i in range(len(labels)):\n",
    "    S[labels[i]] = species(labels[i],tags[i])\n",
    "    \n",
    "for i in list(S.keys()):\n",
    "    S[i].path = ut.check_path(f'opacity/VALD_{S[i].tag}e2/Out_00000_60000_02500_n800.bin',exists=True)\n",
    "    S[i].kappa = jnp.array(ut.read_binary_kitzmann(S[i].path,double=False))\n",
    "    \n",
    "k_wn = jnp.arange(len(S['Fe'].kappa))*1e-2#Wavenumbers\n",
    "k_wl = 1e7/k_wn#Wavelength in nm; common to all the opacity functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138f1a77-9344-4758-b734-b7801a88b117",
   "metadata": {},
   "source": [
    "## Loading a dataset\n",
    "To make a good choice about what to interpolate this onto, we first load in the data. This data is the first transit of KELT-9 b from the Hoeijmakers 2018 paper, observed with HARPS. It's a high-SNR dataset, representative of what you might want to employ high-resolution retrievals on. This code reads spectral orders and wavelengths from file, and generates uncertainties. This is all data prep, and it is copied out of `tayph`, which handles observed spectra in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af785182-b902-44f1-b1de-f39d1ec3b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = ut.check_path('data/KELT-9/night1/',exists=True)#This follows the file structure of tayph.\n",
    "\n",
    "list_of_wls=[]#This will store all the data.\n",
    "list_of_orders=[]\n",
    "list_of_sigmas=[]\n",
    "\n",
    "filelist_orders= [str(i) for i in Path(dp).glob('order_*.fits')]\n",
    "if len(filelist_orders) == 0:#If no order FITS files are found:\n",
    "    raise Exception(f'Runtime error: No orders_*.fits files were found in {dp}.')\n",
    "try:\n",
    "    order_numbers = [int(i.split('order_')[1].split('.')[0]) for i in filelist_orders]\n",
    "except:\n",
    "    raise Exception('Runtime error: Failed at casting fits filename numerals to ints. Are the '\n",
    "    'filenames of all of the spectral orders correctly formatted (e.g. order_5.fits)?')\n",
    "order_numbers.sort()#This is the ordered list of numerical order IDs.\n",
    "n_orders = len(order_numbers)\n",
    "\n",
    "\n",
    "for i in order_numbers:\n",
    "    wavepath = dp/f'wave_{i}.fits'\n",
    "    orderpath= dp/f'order_{i}.fits'\n",
    "    ut.check_path(wavepath,exists=True)\n",
    "    ut.check_path(orderpath,exists=True)\n",
    "    wave_order = ut.readfits(wavepath)#2D or 1D?\n",
    "    order_i = ut.readfits(orderpath)\n",
    "    list_of_wls.append(ops.airtovac(wave_order))# deal with air wavelengths:\n",
    "\n",
    "    #Test for negatives, set them to NaN.\n",
    "    order_i[order_i <= 0] = np.nan #This is very important for later when we are computing\n",
    "    #average spectra and the like, to avoid divide-by-zero cases.\n",
    "    list_of_orders.append(order_i)\n",
    "    list_of_sigmas.append(np.sqrt(order_i))\n",
    "\n",
    "if not batch:   \n",
    "    oi = 6 #This is the order that's going to be plotted below.\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.imshow(list_of_orders[oi],aspect='auto',origin='upper')\n",
    "    plt.title(f'Order # {oi}')\n",
    "    plt.xlabel('Px')\n",
    "    plt.ylabel('Exp')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a1850-e889-4e24-9ba0-ca8c4bd840f1",
   "metadata": {},
   "source": [
    "We now need to do velocity corrections on this data to move to the stellar rest-frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9c30c-bfc6-47d7-be59-e9d4ded68227",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_cor = sp.berv(dp)-sp.RV_star(dp)\n",
    "\n",
    "gamma = 1.0+(rv_cor*u.km/u.s/const.c)#Doppler factor.\n",
    "\n",
    "list_of_orders_cor = []\n",
    "list_of_sigmas_cor = []\n",
    "list_of_wls_cor = []\n",
    "\n",
    "for i in range(len(list_of_wls)):\n",
    "    order = list_of_orders[i]\n",
    "    sigma = list_of_sigmas[i]\n",
    "    order_cor = order*0.0\n",
    "    sigma_cor = sigma*0.0\n",
    "    wl_cor = list_of_wls[i]\n",
    "\n",
    "    for j in range(len(list_of_orders[0])):\n",
    "        order_cor[j] = interp.interp1d(list_of_wls[i]*gamma[j],order[j],bounds_error=False)(wl_cor)\n",
    "        sigma_cor[j] = interp.interp1d(list_of_wls[i]*gamma[j],sigma[j],bounds_error=False)(wl_cor)\n",
    "\n",
    "    list_of_orders_cor.append(order_cor)\n",
    "    list_of_sigmas_cor.append(sigma_cor)\n",
    "    list_of_wls_cor.append(wl_cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415e1f8-9c3d-4a12-a4c6-4c62caced286",
   "metadata": {},
   "source": [
    "## Using the out-of-transit data for creating a synthetic time series\n",
    "\n",
    "Now we need to crop out all the in-transit data from these orders, so that we can deal with uncontaminated, out of transit spectra with which to play. We also need to invent a phase axis. And we crop away most redder orders to avoid tellurics, and to make computations faster as we play around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffca65e-aac8-40e2-b0d7-458d5ec59815",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wl = np.inf #These are initialised and modified below to determine the extreme wavelengths.\n",
    "max_wl = 0\n",
    "\n",
    "\n",
    "mask = sp.transit(dp)\n",
    "mask[mask<1]=0 #We don't want in-transit spectra \n",
    "\n",
    "list_of_orders_oot = []\n",
    "list_of_wls_oot = []\n",
    "list_of_sigmas_oot = []\n",
    "for i in range(order_start,np.min([order_end,len(list_of_orders)])):\n",
    "    list_of_orders_oot.append(list_of_orders_cor[i][mask==1])\n",
    "    list_of_wls_oot.append(list_of_wls_cor[i])\n",
    "    list_of_sigmas_oot.append(list_of_sigmas_cor[i][mask==1])\n",
    "    min_wl = np.min([np.min(list_of_wls_cor[i]),min_wl])\n",
    "    max_wl = np.max([np.max(list_of_wls_cor[i]),max_wl])\n",
    "        \n",
    "list_of_wld = copy.deepcopy(list_of_wls_oot)\n",
    "    \n",
    "if not batch:\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.imshow(list_of_orders_oot[oi],aspect='auto',origin='upper')\n",
    "    plt.title(f'Order # {oi} with in-transit data rejected')\n",
    "    plt.xlabel('Px')\n",
    "    plt.ylabel('Exp')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "n_exp = len(list_of_orders_oot[0])\n",
    "phase = np.linspace(-0.05,0.05,n_exp)#This will be used later to shift the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56896307-c6ff-4a71-ab57-0710685e3683",
   "metadata": {},
   "source": [
    "## Detrending\n",
    "We will now inspect how the time-residuals in this data behave, and see what we need to normalise, or how to detrend e.g. the continuum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b51a6-a17e-4ce5-a496-06d8d4305fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meanfluxes = []#These are the time-dependent average fluxes that we divide out of each order.\n",
    "meanspecs = []#These are the average spectra that we divide out of each order.\n",
    "list_of_res = []\n",
    "list_of_res_e = []\n",
    "\n",
    "for i in range(len(list_of_orders_oot)):\n",
    "    order = list_of_orders_oot[i]\n",
    "    sigma = list_of_sigmas_oot[i]\n",
    "    meanflux = np.nanmean(order,axis=1)\n",
    "    meanfluxes.append(meanflux)\n",
    "    order_norm = (order.T/meanflux).T\n",
    "    sigma_norm = (sigma.T/meanflux).T\n",
    "    meanspec = np.nanmean(order_norm,axis=0)\n",
    "    meanspecs.append(meanspec)\n",
    "    \n",
    "    order_clean = order_norm/meanspec\n",
    "    sigma_clean = sigma_norm/meanspec\n",
    "    \n",
    "    #I'm also going to set NaNs to 1.0 and then set sigma to infinite there.\n",
    "    sigma_clean[np.isfinite(order_clean)==False]=np.inf\n",
    "    order_clean[np.isfinite(order_clean)==False]=1.0\n",
    "    \n",
    "    list_of_res.append(order_clean)\n",
    "    list_of_res_e.append(sigma_clean)\n",
    "\n",
    "    \n",
    "\n",
    "if not batch:\n",
    "    \n",
    "    stdev = np.nanmedian(list_of_res_e[oi]) \n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.imshow(list_of_res[oi],aspect='auto',vmin=1-3*stdev,vmax=1+3*stdev,origin='upper')\n",
    "    plt.title(f'Normalised order # {oi}')\n",
    "    plt.ylabel('Exp')\n",
    "    plt.tight_layout()\n",
    "    plt.show()   \n",
    "    \n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.imshow(list_of_res_e[oi],aspect='auto',origin='upper')\n",
    "    plt.title(f'Normalised uncertainties of order # {oi}')\n",
    "    plt.xlabel('Px')\n",
    "    plt.ylabel('Exp')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    expfit = 11\n",
    "    xfit = np.arange(len(list_of_res[oi][expfit]))\n",
    "    fit = np.polyfit(xfit, list_of_res[oi][expfit], 1,w = 1/list_of_res_e[oi][expfit])\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.plot(xfit,list_of_res[oi][expfit])\n",
    "    plt.plot(xfit,np.poly1d(fit)(xfit))   \n",
    "    plt.title('Fitting residuals with a 1st-order polynomial seems like a good idea')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa90bad-7ff7-4d1f-9ae4-b3a7f0f6bb22",
   "metadata": {},
   "source": [
    "So this data has a skewed continuum. In the ideal world, a slope in the continuum should be part of the model that I am going to fit. But this will generate at least 2 new free parameters (1st degree polynomial) for each exposure in each order. For 70 orders and 30 exposures, this would be over 4000 new free parameters. I do expect that this could  lead to a system of equations that can be solved in the mcmc because all these free parameters are very much decoupled from the physical parameters of the atmosphere (save the continuum degeneracy that's already there), but lets not tackle that problem for the moment. Instead, we fit these separately and save the filter, supply that to the model and filter the model in the same way. This is equivalent to the strategy of Gibson et al. 2022. This is expected to bias the best-fit model somewhat, but I suspect that for spectral features much smaller than the systematic slopes introduced by the instrument, such biases should be small compared to the line depths themselves, which are already tiny. So let's proceed with the filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8c4a6e-1904-4dd0-ae1b-a2d92b55ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_filters = []\n",
    "list_of_res_clean = []\n",
    "list_of_res_clean_e = []\n",
    "deg = 1\n",
    "for i in range(len(list_of_res)):\n",
    "    order = list_of_res[i]\n",
    "    xfit = np.arange(len(order[0]))\n",
    "    polyfilter = order*0.0\n",
    "    fit2d = np.polyfit(xfit,order.T,deg).T\n",
    "\n",
    "    for j in range(len(order)):\n",
    "        polyfilter[j] = np.poly1d(fit2d[j])(xfit)\n",
    "    list_of_filters.append(polyfilter) \n",
    "    list_of_res_clean.append(list_of_res[i]/polyfilter)\n",
    "    list_of_res_clean_e.append(list_of_res_e[i]/polyfilter)\n",
    "\n",
    "    \n",
    "if not batch:       \n",
    "    stdev = np.nanmedian(list_of_res_e[oi])\n",
    "\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.imshow(list_of_filters[oi],aspect='auto',vmin=1-3*stdev,vmax=1+3*stdev,origin='upper')\n",
    "    plt.title(f'The filter applied to order # {oi}')\n",
    "    plt.ylabel('Exp')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.imshow(list_of_res[oi]/list_of_filters[oi],aspect='auto',vmin=1-3*stdev,vmax=1+3*stdev,origin='upper')\n",
    "    plt.title(f'Normalised order # {oi} after colour-detrending')\n",
    "    plt.ylabel('Exp')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f27c566-90ca-47bf-ad32-1239f2e1be3d",
   "metadata": {},
   "source": [
    "Buried in this noise is the transmission spectrum, in units of transit radius, which is our primary observable to which we will fit a model. We will now need to make this model applicable to 2D data, and this filter needs to be applied to the model to account for the change in line depth due to the division of that slope. Because the data is now de-trended and the mean is 1.0 per definition, I will now h-stack the data, the uncertainties and the filters. This will make handling the whole dataset a lot easier.<br><br>\n",
    "\n",
    "Anyway, to apply the filters, I will not divide them out of the data. Instead, I will save these and multiply them against the model. The reason for that is that at some moment, these filters will need to be part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49975db0-d3a2-4841-b7de-e2eb7ffdced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fxd = np.hstack(list_of_res_clean) #This is the data.\n",
    "err = np.hstack(list_of_res_clean_e)#This is the uncertainty\n",
    "fxf = np.hstack(list_of_filters)#This is the filter\n",
    "wld = np.hstack(list_of_wld)#This is the wavelength axis.\n",
    "\n",
    "if not batch:\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.imshow(fxd,aspect='auto',vmin=1-3*stdev,vmax=1+3*stdev)\n",
    "    plt.title(f'All the renormalised, cleaned data')\n",
    "    plt.ylabel('Exp')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.imshow(fxf,aspect='auto',vmin=1-3*stdev,vmax=1+3*stdev)\n",
    "    plt.title(f'The filter that will be passed to the model.')\n",
    "    plt.ylabel('Exp')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa73883f-bcb7-4801-9767-cbc68b4f50ac",
   "metadata": {},
   "source": [
    "Interestingly, the filters all have a very correlated shape from order to order. I think that this makes sense, because colour-variation probably depends on the local shape of the SED, which is very similar order to order. Maybe I can leverage this during the retrieval to make that system of equations smaller? Maybe the polynomial coefficient of each exposure in each order can be related to the coefficients of the other orders via a polynomial themselves? If that's a low order polynomial, the number of free parameters would be a few times the number of exposures; rather than the number of orders times the number of exposures... Not sure that this will generalise though... Perhaps a GP?<br><br><br>\n",
    "\n",
    "Anyway, we now proceed with making the model. The first thing that we're going to do is to prepare the opacities, to convert them to a new wavelength grid that will allow us to blur the resulting transmission spectrum. This again comes straight out of `tayph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6cd48e-9a06-4c94-a3b3-40f9bdd5f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going to bracket the minimum and maximum wavelengths of the intermediate wavelength array by 500 km/s in velocity.\n",
    "doppler_factor = (500*u.km/u.s / const.c).decompose().value\n",
    "min_wl*=(1-doppler_factor)\n",
    "max_wl*=(1+doppler_factor)\n",
    "\n",
    "\n",
    "for i in list(S.keys()):\n",
    "    wli,kappa_i,dv = ops.constant_velocity_wl_grid(np.array(k_wl[1:]),np.array(S[i].kappa[1:]),\n",
    "                                                       oversampling=1.0,minmax=[min_wl,max_wl]) # The intermediate wavelength grid.\n",
    "                                                        #Index it from 1 onwards because the first value is np.inf.\n",
    "    S[i].kappa_i = copy.deepcopy(jnp.array(kappa_i))\n",
    "    \n",
    "wli = jnp.array(copy.deepcopy(wli))    \n",
    "    \n",
    "    \n",
    "kappa_grid = np.vstack([S['Fe'].kappa_i,S['Ti'].kappa_i,S['V'].kappa_i])\n",
    "n_species = len(kappa_grid)#The first species should be Fe.    \n",
    "    \n",
    "    \n",
    "if not batch:\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    for i in list(S.keys()):\n",
    "        plt.plot(wli,S[i].kappa_i,label=i,linewidth=0.5,alpha=0.7)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Wavelength (nm)')\n",
    "    plt.title(\"Opacity functions of various species on our wavelength range\")\n",
    "    plt.ylabel('Opacity (cm$^2$g$^{-1}$)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df923fe-0ede-4934-b654-9a8ff552eaa8",
   "metadata": {},
   "source": [
    "So now we have opacity functions interpolated onto a fine-grained constant-dv grid.\n",
    "The next task is to make a model that matches the order-wise data.  <br><br><br>\n",
    "\n",
    "## Defining the planet and other physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee8d48c-af23-4d42-935c-c8050d6f30c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.57721\n",
    "RJ = const.R_jup.cgs.value\n",
    "MJ = const.M_jup.cgs.value\n",
    "G = const.G.cgs.value\n",
    "Rsun = const.R_sun.cgs.value\n",
    "\n",
    "P0 = (1.0*u.bar).cgs.value#bar\n",
    "R0 = 1.8*RJ\n",
    "M0 = 1.2*MJ\n",
    "k = const.k_B.cgs.value\n",
    "m = 2.33*const.u.cgs.value\n",
    "Rs = 1.4*Rsun\n",
    "g = G*M0 / R0**2\n",
    "\n",
    "c = const.c.to('km/s').value\n",
    "Kp = 200.0#Some test variables.\n",
    "vsys = 20.0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2252c1ed-b64e-4a21-8393-323858760bdf",
   "metadata": {},
   "source": [
    "## The model function\n",
    "\n",
    "Let's specify the model that we will fit to the data using the numpy module within jax. We'll also \"decorate\" it with the `jit` decorator, which will compile the function for us at runtime. \n",
    "\n",
    "This model is complicated for three reasons.<br>\n",
    "First of all, because there is a variety of species. Each of these has their own opacity function and abundance. I set it up such that the first species provided is Fe, and all the other species are then measured relative to Fe.<br>\n",
    "Secondly, the model needs to be broadened and shifted. This adds more free parameters, and a convolution operation.<br>\n",
    "Thirdly, the data is 2D, meaning that the number of datapoints is massive.<br><br><br><br>\n",
    "The smart way to solve the first issue, is to keep the chosen opacities general. So unlike the previous notebook where the species were hardcoded, we now choose to pass the opacity functions as a matrix, ordered such that the first row is the Fe opacity, and the remaining rows are opacity functions that correspond to the order of the abundances relative to Fe, as supplied in the parameter `p` which encodes the free parameters.\n",
    "\n",
    "We define the model in two flavours. The first is the numpy model, which is expected to be slow. The second is the just-in-time compiled model, and we'll compare the performance of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007922f-abbf-46a0-a996-5e4cff52e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_numpy(p,wl,kappa_grid):\n",
    "    #First we unpack our free parameters:\n",
    "    n_species = len(kappa_grid)\n",
    "    T = p[0]\n",
    "    chi_fe = 10**p[1]\n",
    "    logratios = jnp.array(p[2:n_species+1]) #Add a zero for Fe itself. Is this going to mess up autodif?\n",
    "    logk0 = p[n_species+1]\n",
    "    c0 = p[n_species+2] #Constant offset to ensure continuum-degeneracy despite filtering. \n",
    "    \n",
    "    #Then we compute kappa:\n",
    "    chi_i = chi_fe * 10 ** logratios \n",
    "    K = chi_fe * kappa_grid[0] + jnp.dot(chi_i,kappa_grid[1:]) + 10**logk0\n",
    "    \n",
    "    #Then we do the magic:\n",
    "    H = k*T/m/g\n",
    "    R = R0 + H*(gamma+jnp.log(P0 * K / g * jnp.sqrt(2*np.pi*R0/H) ) )\n",
    "    return (c0-R**2 / Rs**2)\n",
    "\n",
    "\n",
    "@jit\n",
    "def model_jax(p,wl,kappa_grid):\n",
    "    #This model is the exact same as above, just with the jit decorator.\n",
    "    n_species = len(kappa_grid)\n",
    "    T = p[0]\n",
    "    chi_fe = 10**p[1]\n",
    "    logratios = jnp.array(p[2:n_species+1])\n",
    "    logk0 = p[n_species+1]\n",
    "    c0 = p[n_species+2]\n",
    "    chi_i = chi_fe * 10 ** logratios \n",
    "    K = chi_fe * kappa_grid[0] + jnp.dot(chi_i,kappa_grid[1:]) + 10**logk0\n",
    "    H = k*T/m/g\n",
    "    R = R0 + H*(gamma+jnp.log(P0 * K / g * jnp.sqrt(2*np.pi*R0/H) ) )\n",
    "    return (c0-R**2 / Rs**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d4d49-488e-4732-9763-ccfa9dbbd547",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not batch:\n",
    "    #The timed comparison:\n",
    "    N=300\n",
    "    p = [2500.0,-5.0,-1.0,-2.0,-2.0,1.0]# The model parameters: T, Fe, 2 other species relative to Fe, log(k0), c0.\n",
    "    t1=ut.start()\n",
    "    for i in range(N):\n",
    "        model_numpy(p,wld,kappa_grid)\n",
    "    t2 = ut.end(t1,silent=True)/N\n",
    "    print(f'Elapsed time per numpy model: {np.round(t2*1000,3)} ms')\n",
    "\n",
    "\n",
    "    t1=ut.start()\n",
    "    for i in range(N):\n",
    "        model_jax(p,wld,kappa_grid)\n",
    "    t2 = ut.end(t1,silent=True)/N\n",
    "    print(f'Elapsed time per jax model: {np.round(t2*1000,3)} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd5fc6-89c5-4a13-9faf-adff717dcd00",
   "metadata": {},
   "source": [
    "If all went well, the jax model is significantly faster (about 3 to 4 times) than numpy. That's great!\n",
    "<br><br><br>\n",
    "\n",
    "## Traced versus static variables\n",
    "\n",
    "This is drop-in type addition of just-in-time compilation and JAX's version of numpy are only the beginning to effectively code with JAX. In JAX, there is a difference between static and traced variables. Traced variables can be seen as the free parameters of a function. Static variables are the constants. <br><br>\n",
    "For example, the function f(x)=a*x + b is commonly understood to have one free parameter and two coefficeints. We could code that up as `def f(x,a,b):...`. However to python and JAX, x, a and b would all be equally free parameters.<br><br>\n",
    "Instead, if a parameter is really <b>not</b> free (for example because they are natural constants that are always the same, no matter in what situation the function is called), they could be hard-coded when the function is compiled. These are <i>static</i> variables in JAX, and it turns out that reducing the number of free function parameters has the potential to make our compiled code faster (see relevant docs [here](https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html)). Variables that define the shape of other various even <b>have</b> to be static variables, because JAX can't compile your function if it doesn't know the shape of the variables in advance.<br><br><br>\n",
    "For our problem, using static variables to try to increase the computation speed may be significant because you can consider the opacity functions of atoms to be natural constants. In fact, the model that we're fitting to the data has a small number of physical free parameters (temperature, abundances), but many things are fixed, meaning that they don't change while the MCMC sampler is traversing likelihood space. That includes the opacities, but also the wavelength grid of the data (which is needed to interpolate onto), some system parameters (g, P0, R0), and the de-trending filter.<br><br><br>\n",
    "Unfortunately, one issue with static variables is that they are not allowed to be numpy arrays or lists. I worked out a way to convert these big 2D matrices to flat byte arrays, which are hashable, and then, passing the dimensions of these arrays as additional function parameters. Inside the function, these byte arrays would then be converted back to numpy arrays, and used in the remainder of the calculation. However, I found out that in the present model, the handling of e.g. the opacity functions is not going to be the slowest operation in this model, so I left that for another day. We will be using some static variables though, but those will be the single constants, and we'll do that later on.<br><br><br>\n",
    "\n",
    "Because first, we still have a lot of other work to do. Importantly, the spectrum needs to be convolved (broadened) to take into account the non-zero-width line-spread function, and it needs to be interpolated onto the data's wavelength array. Let's solve those two problems next.\n",
    "<br><br>\n",
    "\n",
    "## Convolution\n",
    "A very interesting question is <b>what</b> should actually be convolved. The spectrum? The transit radius? Or even the opacity function? In theory, it depends on when various effects are happening. Velocity broadening due to the motion of particles in the atmosphere (or the atmosphere as a whole), may need to be modelled as a convolution of the opacity function itself. However, the effect of the line-spread function of the spectrograph is something that happens to the spectrum that enters the telescope. A model that simply broadens the transit radius or the transmission spectrum is a simplification, and I wonder how other authors deal with that. E.g. Julia Seidel's vertical-wind paper probably shifts the opacity function of each cell when doing the radiative transfer. Of course, the analytical formula is not suitable for this. So for now, we'll keep this as simple as possible.<br><br>\n",
    "We will approach it by first defining the velocity range over which the Gaussian blur kernel needs to be defined. We call this `x_kernel`. This will be defined as a grid of radial velocities, and for each line-width (in km/s), the kernel will have a constant size regardless of wavelength, because the opacity functions are defined on a constant-velocity-step grid. So if we make the sampling of `x_kernel` match that of the grid of $\\kappa$, we can use the standard convolution function of numpy. All we need to do is define `x_kernel` to be wide enough, such that it will be able to contain blur kernels with various widths during the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b635c708-1c60-4f90-9bcb-d1100130dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_conv_numpy(p,wl,kappa_grid,x_kernel):\n",
    "    #First we unpack our free parameters:\n",
    "    n_species = len(kappa_grid)\n",
    "    T = p[0]\n",
    "    chi_fe = 10**p[1]\n",
    "    logratios = jnp.array(p[2:n_species+1]) #Add a zero for Fe itself. Is this going to mess up autodif?\n",
    "    logk0 = p[n_species+1]\n",
    "    c0 = p[n_species+2] #Constant offset to ensure continuum-degeneracy despite filtering. \n",
    "    lw = p[n_species+3]\n",
    "    \n",
    "    #Then we compute kappa:\n",
    "    chi_i = chi_fe * 10 ** logratios \n",
    "    K = chi_fe * kappa_grid[0] + jnp.dot(chi_i,kappa_grid[1:]) + 10**logk0\n",
    "    \n",
    "    #Then we do the magic:\n",
    "    H = k*T/m/g\n",
    "    R = R0 + H*(gamma+jnp.log(P0 * K / g * jnp.sqrt(2*np.pi*R0/H) ) )\n",
    "    \n",
    "    RT = c0-R**2 / Rs**2\n",
    "    \n",
    "    kernel = jnp.exp(-0.5 * x_kernel**2 / lw**2) #/ ( lw*jnp.sqrt(2*np.pi) )\n",
    "#     plt.plot(x_kernel,kernel)\n",
    "#     plt.show() #Looking good.\n",
    "    RT_b = jnp.convolve(RT,kernel/jnp.sum(kernel),mode='same')\n",
    "    return (RT_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde0f203-2f79-4f9b-8dad-2f65dda710fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwli = len(wli)\n",
    "nwld = len(wld)\n",
    "nexp = len(fxf)\n",
    "\n",
    "sfwhm = 2*jnp.sqrt(2*np.log(2))#2.355...\n",
    "\n",
    "\n",
    "#We calculate what range we want for the convolution kernel.\n",
    "mf=5.0#max_fwhm_expected\n",
    "nf=4.0#How many times wider the kernel is compared to the fwhm of the lsf?\n",
    "k_size = int(mf/dv*nf)\n",
    "if k_size%2 == 0: k_size+=1#Make sure that it is odd.\n",
    "x_kernel = (jnp.arange(k_size)-(k_size-1)/2)*dv #This places 0 directly in the middle; and serves as the x axis of our convolution. On-the-fly, this will be used to calculate a gaussian with which to convolve.\n",
    "\n",
    "\n",
    "\n",
    "if not batch:\n",
    "    fx = model_numpy(p,wld,kappa_grid)\n",
    "    p2 = [2500.0,-5.0,-1.0,-2.0,-2.0,1.0,3.0]\n",
    "    fx_b = model_conv_numpy(p2,wld,kappa_grid,x_kernel)\n",
    "    yrange = np.max(fx)-np.min(fx)\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.plot(wli,fx,label='Unbroadened')\n",
    "    plt.plot(wli,fx_b,label='Broadened')\n",
    "    plt.legend()\n",
    "    plt.xlim(390.1,390.5)\n",
    "    plt.xlabel('Wavelength (nm)')\n",
    "    plt.ylabel('Transit depth')\n",
    "    plt.ylim(np.max(fx)-yrange,np.max(fx)+0.3*yrange)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    N=20#We need to decrease the number here, because convolution is slow.\n",
    "    t1=ut.start()\n",
    "    for i in range(N):\n",
    "        model_conv_numpy(p2,wld,kappa_grid,x_kernel)\n",
    "    t2 = ut.end(t1,silent=True)/N\n",
    "    print(f'Elapsed time per numpy model: {np.round(t2*1000,3)} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef9bbc8-2c3e-4c41-ba6d-95eb4ee2e05c",
   "metadata": {},
   "source": [
    "So this convolution has made things some 20x slower, meaning that 95% of the time is spent not calculating a transmission spectrum. In principle this not great news, but it does mean that we don't need to be very worried about the (lack of) accuracy of our analytical model, nor by the 2D-ness of our model later on (which would not increase the time spent in convolution). More complex models may still be bottlenecked by this convolution. <br><br> Next, lets see how long JAX takes here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef37dd7-4d2a-491b-a3f5-6c36759c4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def model_conv_jax(p,wl,kappa_grid,x_kernel):\n",
    "\n",
    "    T = p[0]\n",
    "    chi_fe = 10**p[1]\n",
    "    logratios = jnp.array(p[2:n_species+1]) #Add a zero for Fe itself. Is this going to mess up autodif?\n",
    "    logk0 = p[n_species+1]\n",
    "    c0 = p[n_species+2] #Constant offset to ensure continuum-degeneracy despite filtering. \n",
    "    lw = p[n_species+3]\n",
    "    \n",
    "    #Then we compute kappa:\n",
    "    chi_i = chi_fe * 10 ** logratios \n",
    "    K = chi_fe * kappa_grid[0] + jnp.dot(chi_i,kappa_grid[1:]) + 10**logk0\n",
    "    \n",
    "    #Then we do the magic:\n",
    "    H = k*T/m/g\n",
    "    R = R0 + H*(gamma+jnp.log(P0 * K / g * jnp.sqrt(2*np.pi*R0/H) ) )\n",
    "    \n",
    "    RT = c0 - R**2 / Rs**2\n",
    "    \n",
    "    kernel = jnp.exp(-0.5 * x_kernel**2 / lw**2) #/ ( lw*jnp.sqrt(2*np.pi) )\n",
    "#     plt.plot(x_kernel,kernel)\n",
    "#     plt.show() #Looking good.\n",
    "    RT_b = jnp.convolve(RT,kernel/jnp.sum(kernel),mode='same')\n",
    "    #I explicitly divide by the sum of the kernel instead of doing the analytical normalisation above, \n",
    "    #to deal with cases that the kernel is so wide that it starts going over the edge, reducing the integrated\n",
    "    #area.\n",
    "    \n",
    "    return (RT_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946b17a-d500-440f-b085-ba50b3bc0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not batch:\n",
    "    t1=ut.start()\n",
    "    N=50\n",
    "    for i in range(N):\n",
    "        model_conv_jax(p2,wld,kappa_grid,x_kernel)\n",
    "    t2 = ut.end(t1,silent=True)/N\n",
    "    print(f'Elapsed time per model: {np.round(t2*1000,3)} ms')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce440b-678a-4daf-9f52-743184b3937a",
   "metadata": {},
   "source": [
    "This seems to be a factor of about 2x faster than numpy, and it looks like the convolution still dominates our calculation time.\n",
    "It may be possible to express this convolution as a matrix operation in a smart way later, but for now we'll stick with this. ~50ms for a velocity-broadened model is still very good, because running thousands of models will still be very tractable.<br><br><br>\n",
    "\n",
    "## Expanding the model to two dimensions\n",
    "The next step is to interpolate this model onto the wavelength and phase grid of the data. That goes in two dimensions, and requires the keplerian of the orbit, derived from the orbital phase. We first show how this works outside of the jax function, and then implement it.<br><br>\n",
    "Note that the data wavelength array contains breaks and overlaps. This is not a concern for the interpolation, though, because although `jnp.interp()` requires the 1D spectrum to have monotonically increasing values, the interpolates can be freely chosen. Also note that we do not care about edge effects here. We had chosen the model to be wider than the data, so the interpolated 2D time-series will not have crossed the boundaries of the model.<br><br>\n",
    "In this model we should still divide out the mean of the model, which we have done in the data, but we'll leave that for now. Not doing that should be corrected for by using c0, a constant offset. <br><br>\n",
    "\n",
    "Let's first see step-wise how we are going to do this. First, we take our phase range and multiply it by a large number so that we can visualise more clearly how the planet's spectrum moves along the orbit. We compute the instantaneous radial velocity using the phase, the orbital velocity $K_p$ and a constant offset $v_{sys}$. Then we compute a 1D-model and interpolate it onto an array of doppler-shifted data wavelengths. This is done using an outer product between the doppler shift (as a function of time) and the wavelength axis, creating a 2D grid of wavelengths onto which the model can be interpolated. This takes down three birds with one stroke. First of all, it creates the 2D time series. Second, it doppler shifts the 1D spectrum. Third, it interpolates onto the wavelength grid of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801a531c-de22-4cf3-8f69-420d8c1433d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not batch:\n",
    "    phase_test = phase*6#We artificially increase the phase here to make the sinusoid of the orbit visible.\n",
    "\n",
    "    rvp = jnp.sin(phase_test*2*np.pi)*Kp + vsys#The equation of a circle.\n",
    "    df = 1-rvp/c#The doppler factor associated with these velocities. There is a minus sign here because of the direction of the interpolation.\n",
    "\n",
    "    shifted_wld = jnp.outer(df,wld)#This is a big 2D matrix with shape n_phase x n_wld. So that is as large as the data.\n",
    "\n",
    "    spec = model_conv_jax(p2,wld,kappa_grid,x_kernel)\n",
    "    spec2D = np.interp(shifted_wld,wli,spec)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.plot(wld[3000:5000])\n",
    "    plt.xlabel('Data pixel number')\n",
    "    plt.ylabel('Wavelength of data')\n",
    "    plt.title('Wavelength across an order edge')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.pcolormesh(spec2D[:,0:5000],shading='auto',cmap='magma')\n",
    "    plt.title('The first 5,000 values of the interpolated data spectral time series. Note the break between two orders at x = 4096.')\n",
    "    plt.axvline(4096,linestyle='--',alpha=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3d396-09ff-48b3-ab07-e3976fd91a33",
   "metadata": {},
   "source": [
    "So that is looking great! Even the order-break is interpolated nicely. Implementing this into the JAX function is now very easy. Note that we also choose to change the input of the log-ratios of the elemental abundances, to be a single jnp.array(). Then, in order to start running MCMCs, we will simulate data and perform fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f886d-cd41-4f08-b12b-6a31f70b087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jit, static_argnums=(6,7,8,9,10,11,12,13,14))\n",
    "def model_jax(p,wl,wlk,kappa_grid,x_kernel,phase,c,gamma,k,m,g,P0,R0,Rs2,n_species):\n",
    "#     T,chi_fe,logratios,logk0,c0,lw,vsys,Kp,wl,wlk,kappa_grid,x_kernel,phase,c,gamma,k,m,g,P0,R0,Rs2,n_species = p\n",
    "\n",
    "    T = p[0]\n",
    "    chi_fe = 10**p[1]\n",
    "    logratios = p[2] #This is now a jnp.array().\n",
    "    logk0 = p[3]\n",
    "    c0 = p[4] #Constant offset to ensure continuum-degeneracy despite filtering. \n",
    "    lw = p[5]\n",
    "    vsys = p[6]\n",
    "    Kp = p[7]\n",
    "\n",
    "    #Then we compute kappa:\n",
    "    chi_i = chi_fe * 10 ** logratios \n",
    "    K = chi_fe * kappa_grid[0] + jnp.dot(chi_i,kappa_grid[1:]) + 10**logk0\n",
    "    \n",
    "    #Then we do the magic:\n",
    "    H = k*T/m/g\n",
    "    R = R0 + H*(gamma+jnp.log(P0 * K / g * jnp.sqrt(2*np.pi*R0/H) ) )\n",
    "    RT = c0-R**2 / Rs**2\n",
    "    \n",
    "    #Then we convolve:\n",
    "    kernel = jnp.exp(-0.5 * x_kernel**2 / lw**2)\n",
    "    RT_b = jnp.convolve(RT,kernel/jnp.sum(kernel),mode='same')\n",
    "    \n",
    "    #Then we populate the 2D time series:\n",
    "    rvp = jnp.sin(phase*2*np.pi)*Kp + vsys #Radial velocity of the planet as a function of the orbital phase.\n",
    "    \n",
    "    shifted_wl = jnp.outer(1-rvp/c,wl)#This populates a 2D matrix containing a row of shifted wavelengths for each of the spectra.\n",
    "    spec2D = jnp.interp(shifted_wl,wlk,RT_b) # * filters\n",
    "    \n",
    "    return(spec2D)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def numpyro_model(y,y_e,*args):\n",
    "    \n",
    "    T_prior = numpyro.sample('T', dist.Uniform(low=1500, high=3500))\n",
    "    \n",
    "    chi_fe_prior = numpyro.sample(\n",
    "       'log($\\chi_{Fe}$)', dist.Uniform(low=-5.0, high=-3.0))\n",
    "\n",
    "    chi_species_priors = numpyro.sample('log($\\chi$ / $\\chi_{Fe}$)', dist.Uniform(low=-3.5, high=-0.5),sample_shape=(n_species-1,))\n",
    "\n",
    "    k0_prior = numpyro.sample('log($\\kappa_0$)', dist.Uniform(low=-4.0, high=-1.0))\n",
    "\n",
    "    c_prior = numpyro.sample('c0', dist.Uniform(low=0.998, high=1.002))\n",
    "    \n",
    "    lw_prior = numpyro.sample('lw', dist.Uniform(low=2, high=8))\n",
    "    vsys_prior = numpyro.sample('$v_{sys}$', dist.Uniform(low=16, high=24))\n",
    "    Kp_prior = numpyro.sample('$K_p$', dist.Uniform(low=130, high=170))\n",
    "    \n",
    "    beta = numpyro.sample('$\\\\beta$', dist.Uniform(low=0.5, high=2.0))\n",
    "    \n",
    "    priors = [T_prior,chi_fe_prior,chi_species_priors,k0_prior,c_prior,lw_prior,vsys_prior,Kp_prior]\n",
    "    \n",
    "    # Normally distributed likelihood\n",
    "    numpyro.sample(\"obs\", dist.Normal(loc=model_jax(priors,*args),scale=y_e*beta), obs=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2f7750-b756-4e4b-b4ab-3f3d9469c585",
   "metadata": {},
   "source": [
    "## Defining the data\n",
    "\n",
    "The success of the sampler depends on the amount of data as well as the noise. \n",
    "Above, we have limited the wavelength range of the data to span only a number of orders. \n",
    "The data also came with a fake phase range for the 26 exposures of our dataset. \n",
    "Now below, we define a much smaller phase-range to only sub-sample a small part of our data for now. \n",
    "And we assume various flavours of the noise.\n",
    "<br>\n",
    "First, the 'data' is simply the model plus pure normally distributed noise, with equal standard deviation at all wavelengths.\n",
    "<br>\n",
    "Second, that noise term is made dependent on wavelength, using the standard deviation measured from the data, before.\n",
    "<br>\n",
    "Finally, we inject the model into the data via multiplication with the real data, which gives us real noise, including whatever\n",
    "systematic effects there may be in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ba382-f5e2-4370-875d-18c26b4f9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next step would be to inject the model into the data before cleaning takes place.\n",
    "true_p = [2500.0,-4.0,jnp.array([-1.0,-2.0]),-2.0,1.0,4.0,20.0,150.0]\n",
    "\n",
    "phase = jnp.array([-0.1,-0.05,0,0.05,0.1])\n",
    "true_model = model_jax(true_p,wld,wli,kappa_grid,x_kernel,jnp.array(phase),c,gamma,k,m,g,P0,R0,Rs**2,n_species)\n",
    "\n",
    "#A model with uniform pure gaussian noise.\n",
    "NOISE_1 = 0.01\n",
    "DATA_1 = true_model + np.random.normal(loc=0.0,scale=NOISE_1,size=(len(phase),len(wld)))\n",
    "DATA_1_E = DATA_1*0.0+NOISE_1\n",
    "\n",
    "#A model with pure gaussian noise equal to that measured in the data.\n",
    "#Jax doesn't like infinite values (NaNs, np.inf). I need to replace these:\n",
    "DATA_2 = true_model + np.random.normal(loc=0.0,scale=1.0,size=(len(phase),len(wld))) * err[0:len(phase)]\n",
    "DATA_2_E = np.array(err[0:len(phase)])\n",
    "DATA_2_E[~np.isfinite(DATA_2_E)] = 5.0 # A large error.\n",
    "DATA_2_E = jnp.array(DATA_2_E)\n",
    "\n",
    "\n",
    "#A model injected into real data.\n",
    "DATA_3 = np.array(fxd[0:len(phase)] * true_model)\n",
    "DATA_3[~np.isfinite(DATA_3)] = 1.0\n",
    "DATA_3 = jnp.array(DATA_3)\n",
    "\n",
    "DATA_3_E = copy.deepcopy(DATA_2_E)\n",
    "\n",
    "\n",
    "if not batch:\n",
    "    XLIM = [387,400]\n",
    "    YLIM = [0.9,1.2]\n",
    "    \n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.plot(wld,DATA_1[0],alpha=0.4)\n",
    "    plt.plot(wld,DATA_1[1]+0.05,alpha=0.4)\n",
    "    plt.plot(wld,DATA_1[2]+0.1,alpha=0.4)\n",
    "    plt.plot(wld,true_model[0])\n",
    "    plt.plot(wld,true_model[1]+0.05)\n",
    "    plt.plot(wld,true_model[2]+0.1)\n",
    "    plt.ylim(YLIM[0],YLIM[1])\n",
    "    plt.title('This is what the model and data #1 look like. Straight lines cutting through come from order overlaps. Nothing to worry about.')\n",
    "    plt.xlim(XLIM[0],XLIM[1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()    \n",
    "    \n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.plot(wld,DATA_2[0],alpha=0.4)\n",
    "    plt.plot(wld,DATA_2[1]+0.05,alpha=0.4)\n",
    "    plt.plot(wld,DATA_2[2]+0.1,alpha=0.4)\n",
    "    plt.plot(wld,true_model[0])\n",
    "    plt.plot(wld,true_model[1]+0.05)\n",
    "    plt.plot(wld,true_model[2]+0.1)\n",
    "    plt.ylim(YLIM[0],YLIM[1])\n",
    "    plt.title('Data #2')\n",
    "    plt.xlim(XLIM[0],XLIM[1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.plot(wld,DATA_3[0],alpha=0.4)\n",
    "    plt.plot(wld,DATA_3[1]+0.05,alpha=0.4)\n",
    "    plt.plot(wld,DATA_3[2]+0.1,alpha=0.4)\n",
    "    plt.plot(wld,true_model[0])\n",
    "    plt.plot(wld,true_model[1]+0.05)\n",
    "    plt.plot(wld,true_model[2]+0.1)\n",
    "    plt.ylim(YLIM[0],YLIM[1])\n",
    "    plt.title('Data #3')\n",
    "    plt.xlim(XLIM[0],XLIM[1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "    N=5\n",
    "    t1=ut.start()\n",
    "    for i in range(N):\n",
    "        true_model = model_jax(true_p,wld,wli,kappa_grid,x_kernel,phase,c,gamma,k,m,g,P0,R0,Rs**2,n_species)\n",
    "    t2 = ut.end(t1,silent=True)/N\n",
    "    print(f'Elapsed time per JAX model: {np.round(t2*1000,3)} ms')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea17778-509c-4d6e-af59-12601d0356e2",
   "metadata": {},
   "source": [
    "## Running the MCMC\n",
    "\n",
    "The below sets up the MCMC run, and runs it on one of the versions of the data. DATA_1 with 20 warmup steps and 40 samples runs for about 103 minutes on my laptop. The number of warmup steps are important because they are used by the algorithm to learn how to step through likelihood space. The remaining samples are then used to build up the posterior. <br><br>\n",
    "\n",
    "This cell outputs a statistic of the result, with the retrieved parameters, their uncertainties and also the number of effective samples per parameter (this should ideally be a statistical sample, i.e. hundreds to thousands), as well as a convergence statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a1a58-0e00-4a55-8c85-345ae0c60a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a sampler, using here the No U-Turn Sampler (NUTS)\n",
    "# with a dense mass matrix:\n",
    "sampler = NUTS(\n",
    "    numpyro_model, \n",
    "    dense_mass=True#,    \n",
    "    #target_accept_prob = 0.95,  \n",
    "    #max_tree_depth=7\n",
    ")\n",
    "\n",
    "# Monte Carlo sampling for a number of steps and parallel chains: \n",
    "mcmc = MCMC(\n",
    "    sampler, \n",
    "    num_warmup=20, \n",
    "    num_samples=40, \n",
    "    num_chains=cpu_cores,\n",
    "    chain_method = 'parallel'\n",
    ")\n",
    "\n",
    "# Run the MCMC\n",
    "t1 = ut.start()\n",
    "mcmc.run(rng_keys,DATA_1,DATA_1_E,wld,wli,kappa_grid, x_kernel,phase,c,gamma,k,m,g,P0,R0, Rs**2,n_species)\n",
    "mcmc.print_summary()\n",
    "duration = ut.end(t1)\n",
    "print(f'{np.round(duration/60.0,1)} minutes spent in MCMC.')\n",
    "import pickle as pickle\n",
    "with open('MCMC-result.pkl','wb') as f:\n",
    "        pickle.dump(mcmc, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb093d7a-04e3-4383-aed3-ad3b73811474",
   "metadata": {},
   "source": [
    "## Corner plot\n",
    "\n",
    "If you didn't select too large a number of samples, the below cell can be used to create a corner plot. With 40 samples, the outlines of the posterior distributions should already be well visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c4ae2-a7dd-4afb-a123-91167c733e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run the first cell if you haven't already done so.\n",
    "if not batch:\n",
    "    import pickle5 as pickle\n",
    "    with open('MCMC-result.pkl', 'rb') as inp:\n",
    "        mcmc = pickle.load(inp)\n",
    "\n",
    "#     result = arviz.from_numpyro(mcmc)\n",
    "#     corner(result, quiet=True,truths=truths); \n",
    "\n",
    "\n",
    "    truth_dict = {'T': true_p[0], \n",
    "               'log($\\chi_{Fe}$)': true_p[1], \n",
    "              'log($\\chi$ / $\\chi_{Fe}$)_0' : true_p[2][0],\n",
    "              'log($\\chi$ / $\\chi_{Fe}$)_1' : true_p[2][1],\n",
    "              'log($\\kappa_0$)': true_p[3], \n",
    "              'c0': true_p[4],\n",
    "              'lw': true_p[5],\n",
    "              '$v_{sys}$': true_p[6],\n",
    "              '$K_p$': true_p[7],\n",
    "              '$\\\\beta$' : 1.0\n",
    "             }\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    sample_dict = mcmc.get_samples().items()\n",
    "    samples = []\n",
    "    labels = []\n",
    "    truths = []\n",
    "    \n",
    "    for k,v in sample_dict:\n",
    "        if np.ndim(v) == 1:\n",
    "            samples.append(v)\n",
    "            labels.append(k)\n",
    "            truths.append(truth_dict[k])\n",
    "        else:\n",
    "            for i in range(len(v.T)):\n",
    "                samples.append(v.T[i])\n",
    "                labels.append(k+f'_{i}')\n",
    "                truths.append(np.float(truth_dict[k+f'_{i}']))\n",
    "        \n",
    "    \n",
    "    corner(\n",
    "        np.vstack(samples).T, \n",
    "        quiet=True, \n",
    "        truths=truths,\n",
    "        labels = labels\n",
    "    );\n",
    "    plt.savefig('Posterior.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844932ad-abb4-48a4-9b0c-b0acb2265bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
